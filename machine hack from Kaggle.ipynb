{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Driverless cars are getting closer to reality and at a faster pace than ever. But it is still a bit far fetched dream to have one in your garage. For the time being, there are still a lot of combustion and hybrid cars that roar around the road, for some it chills. Though the overall data on sales of automobiles shows a huge drop in sales in the last couple of years, cars are still a big attraction for many. Cars are more than just a utility for many. They are often the pride and status of the family. We all have different tastes when it comes to owning a car or at least when thinking of owning one.\n",
    "\n",
    "Well here of course as the name suggests we are not concentrating on a new car, rather our interest is in knowing the prices of used cars across the country whether it is a royal l luxury sedan or a cheap budget utility vehicle. In this hackathon, you will be predicting the costs of used cars given the data collected from various sources and distributed across various locations in India.\n",
    "\n",
    "Size of training set: 6,019 records\n",
    "\n",
    "Size of test set: 1,234 records\n",
    "\n",
    "FEATURES:\n",
    "\n",
    "Name: The brand and model of the car.\n",
    "\n",
    "Location: The location in which the car is being sold or is available for purchase.\n",
    "\n",
    "Year: The year or edition of the model.\n",
    "\n",
    "Kilometers_Driven: The total kilometres driven in the car by the previous owner(s) in KM.\n",
    "\n",
    "Fuel_Type: The type of fuel used by the car.\n",
    "\n",
    "Transmission: The type of transmission used by the car.\n",
    "\n",
    "Owner_Type: Whether the ownership is Firsthand, Second hand or other.\n",
    "\n",
    "Mileage: The standard mileage offered by the car company in kmpl or km/kg\n",
    "\n",
    "Engine: The displacement volume of the engine in cc.\n",
    "\n",
    "Power: The maximum power of the engine in bhp.\n",
    "\n",
    "Seats: The number of seats in the car.\n",
    "\n",
    "New_Price: The price of a new car of the same model.\n",
    "\n",
    "Price: The price of the used car in INR Lakhs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel('D:/datasets+minipro/chinmay final project files/machinehack used car sales regression/Data_Train.xlsx')\n",
    "dfte=pd.read_excel('D:/datasets+minipro/chinmay final project files/machinehack used car sales regression/Data_Test.xlsx')\n",
    "df['data']='train'\n",
    "dfte['data']='test'\n",
    "dfte['Price']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Remove the New_price currency unit from each value and convert into relevant amount\n",
    "#     Remove the Engine capacity unit from each value\n",
    "#     Remove the Mileage unit and comvert all \"0.0 kmpl\" value to NaN\n",
    "#     Remove the units of Power and convert \"null bhp\" to NaN\n",
    "#     After all this filtering, we will fill the missing values with the mean or median value respective to each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df['data']='train'\n",
    "dfte['data']='test'\n",
    "dfte['Price']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=pd.concat([df,dfte],axis=0)\n",
    "dff=dff.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff.drop(['Name'],axis=1,inplace=True)\n",
    "dff['brand'] = dff['Name'].apply(lambda x: \" \".join(x.split(' ')[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  problem 1 -Remove the New_price currency unit from each value and convert into relevant amount\n",
    "# Define function to correct the New_Price value\n",
    "def price_correct(x):\n",
    "    if str(x).endswith('Lakh'):\n",
    "        return float(str(x).split()[0])*100000\n",
    "    elif str(x).endswith('Cr'):\n",
    "        return float(str(x).split()[0])*10000000\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff2=dff.copy()\n",
    "# dff2['New_Price']=dff2['New_Price'].apply(price_correct)\n",
    "dff['New_Price']=dff['New_Price'].apply(price_correct)\n",
    "\n",
    "\n",
    "\n",
    "#### Filling New_price missing values with brand each brand and their mean\n",
    "#### Say mazda has mean 15 lakhs,it will replace the mazda with mean of mazda i.e-15 lakhs\n",
    "#### Say toyota has mean 8 lakhs,it will replace the mazda with mean of toyota i.e-8 lakhs\n",
    "\n",
    "\n",
    "dff['New_Price'] = dff.groupby('brand').New_Price.apply(lambda x: x.fillna(x.mean()))\n",
    "dff.New_Price = dff.New_Price.fillna(dff.New_Price.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remove the Engine capacity unit from each value\n",
    "\n",
    "a=dff['Mileage'].str.split(' ',expand=True)\n",
    "\n",
    "a[0]=a[0].astype('float')\n",
    "\n",
    "dff['Mileage']=a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Remove the Engine capacity unit from each value and \n",
    "#   Remove the units of Power and convert \"null bhp\" to NaN\n",
    "# 0 -9 will extract only digits from the values and null bhp will be converted to nan by doing astype,as it automatically \n",
    "# converts null bhp or any string to nan by itself.\n",
    "\n",
    "for i in ['Engine','Power']:\n",
    "    dff[i]=dff[i].str.extract('([0-9]).*',expand=False).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "##  Remove the Mileage unit and convert all \"0.0 kmpl\" value to NaN\n",
    "print(dff[dff['Mileage']==0.0].shape[0])\n",
    "print(dff['Mileage'].isnull().sum())\n",
    "\n",
    "dff['Mileage']=dff['Mileage'].replace({0.0:np.nan})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Power','Mileage','Engine','Seats']:\n",
    "    dff.loc[dff[i].isnull(),i]=dff.loc[dff['data']=='train',i].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2=dff.copy()#Double backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This dff1 is used for visualizations later #########\n",
    "dff1=dff.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       7.845907e+05\n",
       "1       1.183615e+06\n",
       "2       8.610000e+05\n",
       "3       7.845907e+05\n",
       "4       6.514172e+06\n",
       "5       1.183615e+06\n",
       "6       1.161263e+06\n",
       "7       2.100000e+06\n",
       "8       1.191000e+06\n",
       "9       8.775769e+05\n",
       "10      1.065000e+06\n",
       "11      1.166976e+06\n",
       "12      7.845907e+05\n",
       "13      1.193220e+07\n",
       "14      1.193220e+07\n",
       "15      3.201000e+06\n",
       "16      1.166976e+06\n",
       "17      7.845907e+05\n",
       "18      7.676061e+05\n",
       "19      7.364245e+06\n",
       "20      4.787000e+06\n",
       "21      7.845907e+05\n",
       "22      6.514172e+06\n",
       "23      1.183615e+06\n",
       "24      1.191000e+06\n",
       "25      1.166976e+06\n",
       "26      1.161263e+06\n",
       "27      7.845907e+05\n",
       "28      1.057000e+06\n",
       "29      2.362702e+06\n",
       "            ...     \n",
       "7223    2.194577e+06\n",
       "7224    5.530000e+05\n",
       "7225    2.621357e+06\n",
       "7226    1.183615e+06\n",
       "7227    1.103028e+06\n",
       "7228    7.060000e+05\n",
       "7229    1.191000e+06\n",
       "7230    1.183615e+06\n",
       "7231    8.790000e+05\n",
       "7232    1.816000e+06\n",
       "7233    6.514172e+06\n",
       "7234    1.183615e+06\n",
       "7235    1.057000e+06\n",
       "7236    7.080000e+05\n",
       "7237    1.166976e+06\n",
       "7238    6.514172e+06\n",
       "7239    1.166976e+06\n",
       "7240    1.166976e+06\n",
       "7241    6.247333e+06\n",
       "7242    1.183615e+06\n",
       "7243    7.676061e+05\n",
       "7244    2.194577e+06\n",
       "7245    1.166976e+06\n",
       "7246    1.183615e+06\n",
       "7247    1.183615e+06\n",
       "7248    1.191000e+06\n",
       "7249    1.191000e+06\n",
       "7250    1.161263e+06\n",
       "7251    1.191000e+06\n",
       "7252    7.364245e+06\n",
       "Name: New_Price, Length: 7253, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff1.to_csv(\"D:/datasets+minipro/chinmay final project files/machinehack used car sales regression/carscombined.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location\n",
      "Fuel_Type\n",
      "Transmission\n",
      "Owner_Type\n"
     ]
    }
   ],
   "source": [
    "# Encoding!\n",
    "# grabbing only categorical columns,so we can filter out only imp categorical features with high value counts\n",
    "# this is something that we need to do because, low value_counts() observation will not contribute much to machine learning model\n",
    "\n",
    "cat_cols=dff.select_dtypes(include='object').columns\n",
    "cat_cols=['Location', 'Fuel_Type', 'Transmission', 'Owner_Type']\n",
    "\n",
    "# Removing the infrequent responses from the column ie >10 and removing them \n",
    "for col in cat_cols:\n",
    "    freqs=dff[col].value_counts()\n",
    "    k=freqs.index[freqs>10][:-1]    # checking atleast 10 responses, 20 comes after checking the data\n",
    "    for cat in k:\n",
    "        name=col+'_'+cat\n",
    "        dff[name]=(dff[col]==cat).astype(int)\n",
    "    del dff[col]\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine                    0\n",
       "Kilometers_Driven         0\n",
       "Mileage                   0\n",
       "Name                      0\n",
       "New_Price                 0\n",
       "Power                     0\n",
       "Price                  1234\n",
       "Seats                     0\n",
       "Year                      0\n",
       "data                      0\n",
       "brand                     0\n",
       "Location_Mumbai           0\n",
       "Location_Hyderabad        0\n",
       "Location_Kochi            0\n",
       "Location_Coimbatore       0\n",
       "Location_Pune             0\n",
       "Location_Delhi            0\n",
       "Location_Kolkata          0\n",
       "Location_Chennai          0\n",
       "Location_Jaipur           0\n",
       "Location_Bangalore        0\n",
       "Fuel_Type_Diesel          0\n",
       "Fuel_Type_Petrol          0\n",
       "Fuel_Type_CNG             0\n",
       "Transmission_Manual       0\n",
       "Owner_Type_First          0\n",
       "Owner_Type_Second         0\n",
       "Owner_Type_Third          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.isnull().sum() # the price column in which null values are present are from test data!\n",
    "# so dont worry about the 1234 null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.drop(['Name','brand'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating train and test data back to their original dataframes.\n",
    "\n",
    "df=dff[dff['data']=='train']\n",
    "del df['data']\n",
    "\n",
    "dfte=dff[dff['data']=='test']\n",
    "dfte.drop(['Price','data'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My temp testing part 1[Remove only this section later:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IQR method to remove or trace the outliers.\n",
    "q1=df['Price'].quantile(.25)\n",
    "q2=df['Price'].quantile(0.50)\n",
    "q3=df['Price'].quantile(0.75)\n",
    "IQR=q3-q1\n",
    "ll=q1-1.5*IQR\n",
    "ul=q3+1.5*IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price'][(df['Price']<ll) | (df['Price']>ul)].shape[0] # there are 718 outliers\n",
    "print('outlier count for price in original df training : {}'.format(df['Price'][(df['Price']<ll) | (df['Price']>ul)].shape[0])) # so for dfcp,we get 0 outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcp=df[~((df['Price']<ll) | (df['Price']>ul))] # negated  outliers to dfcp,no outliers in dfcp\n",
    "print('outlier count for price in dfcp : {}'.format(dfcp['Price'][(dfcp['Price']<ll) | (dfcp['Price']>ul)].shape[0])) # so for dfcp,we get 0 outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(1,2,figsize=(9,3))\n",
    "sns.distplot(df['Price'],ax=axes[0])\n",
    "plt.title('Plot of actual df Price')\n",
    "sns.distplot(dfcp['Price'],ax=axes[1]) # plot of dfcp appears to be more normally distributed,but,we wont be able to get the prices\n",
    "#which are in high range,so we have to keep the outliers\n",
    "plt.title('dfcp plot of price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My temp testing for part 2: assumption of lin reg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df.drop(['Price','Location_Mumbai','Location_Jaipur','Location_Pune','Location_Kolkata',\n",
    "#             'Owner_Type_First','Kilometers_Driven','Location_Kochi','Owner_Type_Second','Location_Delhi','Location_Chennai'], axis=1)\n",
    "import statsmodels.api as sm\n",
    "X = df.drop(['Price'], axis=1)\n",
    "y = df['Price']\n",
    "Xc=sm.add_constant(X)\n",
    "model = sm.OLS(y, Xc).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumption 1: No autocorrelation:\n",
    "import statsmodels.tsa.api as smt\n",
    "acf = smt.graphics.plot_acf(model.resid, lags=30 , alpha=0.05)\n",
    "acf.show()\n",
    "# So, we can see that all lines are within blue boundary indicating that there is almost no autocorrelation\n",
    "# between input variables.\n",
    "# Also,durbin watson value is closer to 2 meaning that there is no autocorrelation.\n",
    "# To summarize:\n",
    "#      0-2 : +ve autocorrelation\n",
    "#      closer to 2 : Little or No Autocorrelation\n",
    "#      2-4 : -ve autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption 2 : Normality of residuals:[using jarque berra test]:\n",
    "from scipy import stats\n",
    "print(stats.jarque_bera(model.resid))\n",
    "print('chisquare stat critical',stats.chi2.isf(0.05,2))\n",
    "\n",
    "# Framing hypothesis:\n",
    "# H0- Errors/residual are normally distributed\n",
    "# HA- Errors/residuals are not normally distributed\n",
    "## from output of p value as p-val < alpha, we reject null hypothesis and accept alternate hypothesis.\n",
    "# we can say that errors are not normally distributed with statistical certainty.\n",
    "# Also, from JB statistic,we can see that it is much higher than 6,so it definitely lies in critical region,So,\n",
    "# Reject null hypothesis there as well.\n",
    "sns.distplot(model.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption 3: Linearity of residuals:\n",
    "import statsmodels.api as sm\n",
    "print(sm.stats.diagnostic.linear_rainbow(res=model, frac=0.5))\n",
    "# H0 -errors are linear \n",
    "# HA- errors are not linear\n",
    "# So, p value < alpha ,we reject null hypothesis,i.e- errors/residual are not normal.\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "import pylab\n",
    "st_residual = model.get_influence().resid_studentized_internal\n",
    "stats.probplot(st_residual, dist=\"norm\", plot = pylab)\n",
    "plt.show()\n",
    "\n",
    "# Also,the graph shows plot is not following linearity.\n",
    "# As, p value is less than alpha,we reject null hypothesis.So,we can say with statistical certainty that\n",
    "# errors are not linear which means that linear models will not work efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption 4 : homoscedastic\n",
    "# H0- Data is homoscedastic [ constant variance across the data] \n",
    "# HA- Data is heteroscedastic [No constant variance across the data] \n",
    "from statsmodels.compat import lzip\n",
    "import statsmodels.stats.api as sms\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(model.resid, model.model.exog)\n",
    "lzip(name, test)\n",
    "# Since,P value > alpha ,we fail to reject null hypothesis.\n",
    "# here,we can say that data is homoscedastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption 5- Multicollinearity:\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(Xc.values, i) for i in range(Xc.shape[1])]\n",
    "pd.DataFrame({'vif': vif[1:]}, index=X.columns).T\n",
    "\n",
    "## So,Multicollinearity exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From all the above assumptions are failure for linear model except variance being equally distrubuted.\n",
    "# We can firmly say that linear model is not applicable here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My temp testing for data part 3[ doing feature selection bakward elimination ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfback=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfback.drop(['Price'], axis=1)\n",
    "y = dfback['Price']\n",
    "#Adding constant column of ones, mandatory for sm.OLS model\n",
    "X_1 = sm.add_constant(X)\n",
    "#Fitting sm.OLS model\n",
    "model = sm.OLS(y,X_1).fit()\n",
    "model.pvalues\n",
    "# Try backward elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backward Elimination\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "selected_features_BE = cols\n",
    "print('Final list of selected variables using backward elimination loop are: ',selected_features_BE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My temp testing for data part 4: \n",
    "# Visualiztion part trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dff1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "sns.countplot(dff1['brand'])\n",
    "plt.show()\n",
    "\n",
    "# As ,we can see from countplot given below,Number of People purchasing Maruti is quite high,Following it closely \n",
    "# is Hyndai car brand.\n",
    "# The Cars which are least sold are from smart to Opelcorsa brand.This may be the case because the cars which are sold\n",
    "# by this brand may be only sports car or showoff type of cars which are very expensive!\n",
    "# So people in middle income group will not go for this car.\n",
    "# they will only go for purchasing the cars which are economical in nature,meaning they are looking for a car which will\n",
    "# last for a long time with least maintainance cost.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,axes=plt.subplots(1,3,figsize=(15,5))\n",
    "# sns.countplot(dff1['Fuel_Type'],ax=axes[0])\n",
    "# sns.countplot(dff1['brand'],ax=axes[1])\n",
    "# plt.show()\n",
    "sns.countplot(dff1['Fuel_Type'])\n",
    "plt.show()\n",
    "# Inference:\n",
    "# From below count plot we can infer that people with mid to high income group are more into purchasing cars\n",
    "# with fuel type as diesel/petrol rather than LPG/CNG/Electric[Maybe Sports car]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(dff1['Mileage'],dff1['Price'],hue=dff1['Fuel_Type'])\n",
    "plt.show()\n",
    "# The sparsed data between Price and mileage confirms that people are buying more cars with fuel type as petrol/diesel\n",
    "# We can also notice that Few cars are giving high mileage with very extreme price with diesel as type.\n",
    "# For some reason,CNG care giving high mileage but people are not interested in buying these types of cars.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('People buying cars with petrol type:\\n',dff1['Location'][dff1['Fuel_Type']=='Petrol'].value_counts(normalize=True)*100)\n",
    "print('People buying cars with diesel type\\n',dff1['Location'][dff1['Fuel_Type']=='Diesel'].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(1,4,figsize=(15,5))\n",
    "sns.scatterplot(dff1['Location'][dff1['Fuel_Type']=='LPG'],dff1['Price'],hue=dff1['Fuel_Type'][dff1['Fuel_Type']=='LPG'],ax=axes[0])\n",
    "plt.xticks(rotation=90)\n",
    "sns.scatterplot(dff1['Location'][dff1['Fuel_Type']=='Petrol'],dff1['Price'],hue=dff1['Fuel_Type'][dff1['Fuel_Type']=='Petrol'],ax=axes[1])\n",
    "plt.xticks(rotation=90)\n",
    "sns.scatterplot(dff1['Location'][dff1['Fuel_Type']=='Diesel'],dff1['Price'],hue=dff1['Fuel_Type'][dff1['Fuel_Type']=='Diesel'],ax=axes[2])\n",
    "plt.xticks(rotation=90)\n",
    "sns.scatterplot(dff1['Location'][dff1['Fuel_Type']=='Electric'],dff1['Price'],hue=dff1['Fuel_Type'][dff1['Fuel_Type']=='Electric'],ax=axes[3])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# As we can observe from below shown scatterplot,\n",
    "# People from hyderabad are more into buying LPG type of cars even if they are expensive.huh!\n",
    "# Mumbai is on top roughly 16% for buying cars with petrol as the fuel type.not only that but people from mumbai have also \n",
    "# bought the cars which are electric cars which are very expensive as compared to other cars!\n",
    "# Hyderabad is on top for buying cars with diesel as fuel type.roughly 15% people have bough from hyderabad only\n",
    "# so,we can notice that people from mumbai are much diverse income wise because it is the only city  which \n",
    "# has bought all different types of cars with each and every variation from economical cars to  expensive cars.\n",
    "# so,Car selling marketing campaign should be more targeted or focused  towards mumbai and hyderabad \n",
    "# and then the rest of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Engine','Seats','Transmission','Owner_Type']:\n",
    "    sns.countplot(dff1[i])\n",
    "    plt.show()\n",
    "# People are more inclined to buy a car which has only 1 engine or even 2.\n",
    "# The Cars with 5 seats are sold more,seems logical..As these are people from middle to higher income class :)\n",
    "# These are the cars with manual tranmission control.\n",
    "# 4th countplot tells you about the ownership of the car which is previously owned or new or something else.\n",
    "# People mostly prefer first hand cars as compared to 2nd hand cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Applying ML Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from scipy.stats import ttest_ind, ttest_1samp, levene, f_oneway, bartlett, shapiro, mannwhitneyu\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Price', axis=1)\n",
    "Xc=sm.add_constant(X)\n",
    "y = df['Price']\n",
    "lr = LinearRegression()\n",
    "model = lr.fit(X, y)\n",
    "model.score(X, y)\n",
    "y_pred = model.predict(X)\n",
    "print('R² of data:', r2_score(y, y_pred))\n",
    "print('RMSE of data:', np.sqrt(mean_squared_error(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Price', axis=1)\n",
    "Xc=sm.add_constant(X)\n",
    "y = df['Price']\n",
    "\n",
    "\n",
    "#Splitting X&y using train_test:\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=8)\n",
    "\n",
    "## only linear regression:\n",
    "\n",
    "linreg=LinearRegression()\n",
    "linreg.fit(X_train,y_train)\n",
    "y_tr_pred=linreg.predict(X_train)\n",
    "y_test_pred=linreg.predict(X_test)\n",
    "\n",
    "# training data & testing data accuracy:\n",
    "print('r2_score on training data: ',r2_score(y_train,y_tr_pred)) #train\n",
    "print('r2_score on testing data: ',r2_score(y_test,y_test_pred)) #test\n",
    "\n",
    "# RMSE for train & test:\n",
    "print('RMSE on training data: ',np.sqrt(mean_squared_error(y_train,y_tr_pred)))  #train\n",
    "print('RMSE on testing data: ',np.sqrt(mean_squared_error(y_test,y_test_pred))) #test\n",
    "\n",
    "\n",
    "# The model seems to be overfit for linear regression as difference between training and testing accuracy is very high!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression base model (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using only those variables which are giving us good results.\n",
    "X = df.drop(['Price','Location_Mumbai','Location_Jaipur','Location_Pune','Location_Kolkata',\n",
    "             'Owner_Type_First','Kilometers_Driven','Location_Kochi','Owner_Type_Second','Location_Delhi','Location_Chennai'], axis=1)\n",
    "y = df['Price']\n",
    "Xc=sm.add_constant(X)\n",
    "model = sm.OLS(y, Xc).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-KNN regressor: [ Scaling is needed as it deals with distance ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Price','Location_Mumbai','Location_Jaipur','Location_Pune','Location_Kolkata',\n",
    "             'Owner_Type_First','Kilometers_Driven','Location_Kochi','Owner_Type_Second','Location_Delhi','Location_Chennai'], axis=1)\n",
    "y = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "knn_params = {'n_neighbors':np.arange(3,20), 'weights':['uniform','distance']}\n",
    "gscv = GridSearchCV(knn, knn_params, cv=3, scoring='neg_mean_squared_error')\n",
    "X_scaled = sc.fit_transform(X)\n",
    "gscv.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best=gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(**knn_best)\n",
    "kfold = KFold(shuffle=True, n_splits=3, random_state=0)\n",
    "cv_results = cross_val_score(knn, X_scaled, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print('%s : %f(%f)' %('KNeighborsRegressor:',np.mean(np.sqrt(np.abs(cv_results))), np.var(np.sqrt(np.abs(cv_results)),ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsRegressor(**knn_best)\n",
    "knn.fit(X,y)\n",
    "knn.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(**knn_best)\n",
    "knn.fit(X_train,y_train)\n",
    "y_tr_pred=knn.predict(X_train)\n",
    "y_test_pred=knn.predict(X_test)\n",
    "\n",
    "# training data & testing data accuracy:\n",
    "print('r2_score on training data: ',r2_score(y_train,y_tr_pred)) #train\n",
    "print('r2_score on testing data: ',r2_score(y_test,y_test_pred)) #test\n",
    "\n",
    "# RMSE for train & test:\n",
    "print('RMSE on training data: ',np.sqrt(mean_squared_error(y_train,y_tr_pred)))  #train\n",
    "print('RMSE on testing data: ',np.sqrt(mean_squared_error(y_test,y_test_pred))) #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([y_test.reset_index(drop=True),pd.DataFrame(y_test_pred,columns=['y_test_pred'])],axis=1).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Decision Tree regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Price','Location_Mumbai','Location_Jaipur','Location_Pune','Location_Kolkata',\n",
    "             'Owner_Type_First','Kilometers_Driven','Location_Kochi','Owner_Type_Second','Location_Delhi','Location_Chennai'], axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=0)\n",
    "dt_params = {'max_depth':np.arange(1,15), 'min_samples_leaf':np.arange(2,15)}\n",
    "rscv_dt =RandomizedSearchCV(dt, dt_params, cv=3, scoring='neg_mean_squared_error')\n",
    "rscv_dt.fit(X, y)\n",
    "print(rscv_dt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best=rscv_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeRegressor(**dt_best)\n",
    "\n",
    "dt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=18)\n",
    "\n",
    "\n",
    "dt=DecisionTreeRegressor(**dt_best)\n",
    "\n",
    "dt.fit(X_train,y_train)\n",
    "y_tr_pred=dt.predict(X_train)\n",
    "y_test_pred=dt.predict(X_test)\n",
    "\n",
    "# training data & testing data accuracy:\n",
    "print('r2_score on training data: ',r2_score(y_train,y_tr_pred)) #train\n",
    "print('r2_score on testing data: ',r2_score(y_test,y_test_pred)) #test\n",
    "\n",
    "# RMSE for train & test:\n",
    "print('RMSE on training data: ',np.sqrt(mean_squared_error(y_train,y_tr_pred)))  #train\n",
    "print('RMSE on testing data: ',np.sqrt(mean_squared_error(y_test,y_test_pred))) #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([y_test.reset_index(drop=True),pd.DataFrame(y_test_pred,columns=['y_test_pred'])],axis=1).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Random Forest Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Price','Location_Mumbai','Location_Jaipur','Location_Pune','Location_Kolkata',\n",
    "             'Owner_Type_First','Kilometers_Driven','Location_Kochi','Owner_Type_Second','Location_Delhi','Location_Chennai'], axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "mse_bias= []\n",
    "mse_var=[]\n",
    "for n_e in np.arange(5,30):\n",
    "    RF=RandomForestRegressor(n_estimators=n_e,random_state=0)\n",
    "    kfold = KFold(shuffle=True, n_splits=3, random_state=0)\n",
    "    mse = cross_val_score(RF, X_scaled, y, cv=kfold, scoring='neg_mean_squared_error' )\n",
    "    mse_bias.append(np.mean(np.sqrt(np.abs(mse))))\n",
    "    mse_var.append(np.var(np.sqrt(np.abs(mse)), ddof=1))\n",
    "print('Min Bias Error:',np.min(mse_bias),' n_estimator:',np.argmin(mse_bias)+1,' Variance Error:',mse_var[np.argmin(mse_bias)])\n",
    "print('Bias Error:',mse_bias[np.argmin(mse_var)],' n_estimator:',np.argmin(mse_var)+1,'Min Variance Error:',np.min(mse_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestRegressor(n_estimators=20,random_state=0)\n",
    "RF.fit(X,y)\n",
    "RF.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestRegressor(n_estimators=20,random_state=0)\n",
    "RF.fit(X_train,y_train)\n",
    "y_tr_pred=RF.predict(X_train)\n",
    "y_test_pred=RF.predict(X_test)\n",
    "\n",
    "# training data & testing data accuracy:\n",
    "print('r2_score on training data: ',r2_score(y_train,y_tr_pred)) #train\n",
    "print('r2_score on testing data: ',r2_score(y_test,y_test_pred)) #test\n",
    "\n",
    "# RMSE for train & test:\n",
    "print('RMSE on training data: ',np.sqrt(mean_squared_error(y_train,y_tr_pred)))  #train\n",
    "print('RMSE on testing data: ',np.sqrt(mean_squared_error(y_test,y_test_pred))) #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([y_test.reset_index(drop=True),pd.DataFrame(y_test_pred,columns=['y_test_pred'])],axis=1).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Ada boosting DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeRegressor(**dt_best)\n",
    "AB_DT = AdaBoostRegressor(base_estimator=dt, random_state=0)\n",
    "mse_bias= []\n",
    "mse_var=[]\n",
    "for n_e in np.arange(5,13):\n",
    "    AB_DT=AdaBoostRegressor(n_estimators=n_e,random_state=0)\n",
    "    kfold = KFold(shuffle=True, n_splits=3, random_state=0)\n",
    "    mse = cross_val_score(AB_DT, X, y, cv=kfold, scoring='neg_mean_squared_error' )\n",
    "    mse_bias.append(np.mean(np.sqrt(np.abs(mse))))\n",
    "    mse_var.append(np.var(np.sqrt(np.abs(mse)), ddof=1))\n",
    "print(np.min(mse_bias))\n",
    "print(np.argmin(mse_bias))\n",
    "print(np.min(mse_var))\n",
    "print(np.argmin(mse_var))\n",
    "\n",
    "print('Min Bias Error:',np.min(mse_bias),' n_estimator:',np.argmin(mse_bias)+1,' Variance Error:',mse_var[np.argmin(mse_bias)])\n",
    "print('Bias Error:',mse_bias[np.argmin(mse_var)],' n_estimator:',np.argmin(mse_var)+1,'Min Variance Error:',np.min(mse_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Ada boosted RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestRegressor(n_estimators=3,random_state=1)\n",
    "AB_RF = AdaBoostRegressor(base_estimator=RF,random_state=0)\n",
    "mse_bias= []\n",
    "mse_var=[]\n",
    "for n_e in np.arange(5,30):\n",
    "    AB_RF=AdaBoostRegressor(n_estimators=n_e,random_state=0)\n",
    "    kfold = KFold(shuffle=True, n_splits=3, random_state=0)\n",
    "    mse = cross_val_score(AB_RF, X, y, cv=kfold, scoring='neg_mean_squared_error' )\n",
    "    mse_bias.append(np.mean(np.sqrt(np.abs(mse))))\n",
    "    mse_var.append(np.var(np.sqrt(np.abs(mse)), ddof=1))\n",
    "print('Min Bias Error:',np.min(mse_bias),' n_estimator:',np.argmin(mse_bias)+1,' Variance Error:',mse_var[np.argmin(mse_bias)])\n",
    "print('Bias Error:',mse_bias[np.argmin(mse_var)],' n_estimator:',np.argmin(mse_var)+1,'Min Variance Error:',np.min(mse_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost same bias and variance,so we wont use adaboosted dt/RF regressor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Gradient Boosting: [ Uses DT by default ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(['Price','Location_Mumbai','Location_Jaipur','Location_Pune','Location_Kolkata',\n",
    "#              'Owner_Type_First','Kilometers_Driven','Location_Kochi','Owner_Type_Second','Location_Delhi','Location_Chennai'], axis=1)\n",
    "# y = df['Price']\n",
    "\n",
    "# mse_var=[]\n",
    "# mse_bias = []\n",
    "# GBoost=GradientBoostingRegressor()\n",
    "# for ne in np.arange(1,20):\n",
    "#     kfold = KFold(shuffle=True,n_splits=3,random_state=0)\n",
    "#     mse = cross_val_score(GBoost, X_scaled, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "#     mse_var=[].append(np.var(np.sqrt(np.abs(mse)), ddof=1))\n",
    "#     mse_bias.append(np.mean(np.sqrt(np.abs(mse))))\n",
    "    \n",
    "# print('Min Bias Error:',np.min(mse_bias),' n_estimator:',np.argmin(mse_bias)+1,' Variance Error:',mse_var[np.argmin(mse_bias)])\n",
    "# print('Bias Error:',mse_bias[np.argmin(mse_var)],' n_estimator:',np.argmin(mse_var)+1,'Min Variance Error:',np.min(mse_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrain.shape)\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytrain.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #set seed for same results everytime\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import sklearn.metrics as metrics\n",
    "# seed=0\n",
    "\n",
    "# X = df.drop(['Price','Location_Mumbai','Location_Jaipur','Location_Pune','Location_Kolkata',\n",
    "#              'Owner_Type_First','Kilometers_Driven','Location_Kochi','Owner_Type_Second','Location_Delhi','Location_Chennai'], axis=1)\n",
    "# y = df['Price']\n",
    "\n",
    "\n",
    "# xtrain,ytrain,xtest,ytest=train_test_split(X,y,test_size=.2,random_state=seed)\n",
    "\n",
    "# #declare the models\n",
    "# lr=LinearRegression()\n",
    "# dt=DecisionTreeRegressor()\n",
    "# rf=RandomForestRegressor(random_state=seed)\n",
    "# bgc=BaggingRegressor()\n",
    "# adb=AdaBoostRegressor()\n",
    "# gbc=GradientBoostingRegressor()\n",
    "\n",
    "# #create a list of models\n",
    "# models=[lr,dt,rf,bgc,adb,gbc]\n",
    "\n",
    "# def score_ensemble_model(xtrain,ytrain,xtest,ytest):\n",
    "#     mod_columns=[]\n",
    "#     mod=pd.DataFrame(columns=mod_columns)\n",
    "#     i=0\n",
    "#     #read model one by one\n",
    "#     for model in models:\n",
    "#         model.fit(xtrain,ytrain)\n",
    "#         y_pred=model.predict(xtest)\n",
    "        \n",
    "#         #compute metrics\n",
    "#         train_accuracy=model.score(xtrain,ytrain)\n",
    "#         test_accuracy=model.score(xtest,ytest)        \n",
    "#         #insert in dataframe\n",
    "#         mod.loc[i,\"Model_Name\"]=model.__class__.__name__\n",
    "#         mod.loc[i,\"Train_Accuracy\"]=round(train_accuracy,2)\n",
    "#         mod.loc[i,\"Test_Accuracy\"]=round(test_accuracy,2)\n",
    "#         mod.loc[i,'r2_score'] = metrics.r2_score(ytest,y_pred)\n",
    "    \n",
    "#         i+=1\n",
    "    \n",
    "#     #sort values by accuracy\n",
    "#     mod.sort_values(by=['r2_score'],ascending=False,inplace=True)\n",
    "#     return(mod)\n",
    "\n",
    "\n",
    "# ##### Apply the function:\n",
    "# ensemble_report=score_ensemble_model(xtrain,ytrain,xtest,ytest)\n",
    "# ensemble_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combined result of all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=LinearRegression()\n",
    "KNN = KNeighborsRegressor(**knn_best)\n",
    "DT =DecisionTreeRegressor(**dt_best)\n",
    "RF = RandomForestRegressor(n_estimators=20,random_state=0,criterion='mae')\n",
    "AB_RF = AdaBoostRegressor(base_estimator=RF,n_estimators=2,random_state=0)\n",
    "AB_DT = AdaBoostRegressor(base_estimator=DT,n_estimators=2,random_state=0)\n",
    "# GBoost = GradientBoostingRegressor(n_estimators=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LinearRegressor',LR))\n",
    "models.append(('KNNRegressor',KNN))\n",
    "models.append(('DT_Regressor',DT))\n",
    "models.append(('RF_Regressor',RF))\n",
    "models.append(('AdaBoostRF',AB_RF))\n",
    "models.append(('AdaBoostDT',AB_DT))\n",
    "# models.append(('GradientBoostRegressor',GBoost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(shuffle=True,n_splits=3,random_state=0)\n",
    "    cv_results = cross_val_score(model, X_scaled, y,cv=kfold, scoring='neg_mean_squared_error')\n",
    "    results.append(np.sqrt(np.abs(cv_results)))\n",
    "    names.append(name)\n",
    "    print(\"%s: %f (%f)\" % (name, np.mean(np.sqrt(np.abs(cv_results))),np.var(np.sqrt(np.abs(cv_results)),ddof=1)))\n",
    "   # boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression is giving us the worst results.This is the reason why it fails every assumption for linear regression\n",
    "# and we go for non linear modelling because its better to use non linear model,instead of transforming the target variable\n",
    "# the making it linear and then applying linear regression.So,Its always better to go with non linear models.!\n",
    "# As ,we can observe from above comparison DT is giving us best results in terms of bias and variance.\n",
    "# So,we can use RF as our best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final prediction on test dataset using DT and RF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample data testing:\n",
    "# We choose best model as Random forest because,even though it is best in terms of bias and variance error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest on our original data using it as training and validation data.\n",
    "\n",
    "RF=RandomForestRegressor(n_estimators=20,random_state=0)\n",
    "RF.fit(X_train,y_train)\n",
    "y_tr_pred=RF.predict(X_train)\n",
    "y_test_pred=RF.predict(X_test)\n",
    "\n",
    "# training data & testing data accuracy:\n",
    "print('r2_score on training data: ',r2_score(y_train,y_tr_pred)) #train\n",
    "print('r2_score on testing data: ',r2_score(y_test,y_test_pred)) #test\n",
    "\n",
    "# RMSE for train & test:\n",
    "print('RMSE on training data: ',np.sqrt(mean_squared_error(y_train,y_tr_pred)))  #train\n",
    "print('RMSE on testing data: ',np.sqrt(mean_squared_error(y_test,y_test_pred))) #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the other columns from the test file to match trained data features.\n",
    "dfte=dfte.drop(['Location_Mumbai','Location_Jaipur','Location_Pune','Location_Kolkata',\n",
    "                'Owner_Type_First','Kilometers_Driven','Location_Kochi','Owner_Type_Second',\n",
    "                'Location_Delhi','Location_Chennai'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  using RF:\n",
    "\n",
    "X_trainf = df.drop(['Price','Location_Mumbai','Location_Jaipur','Location_Pune','Location_Kolkata',\n",
    "             'Owner_Type_First','Kilometers_Driven','Location_Kochi','Owner_Type_Second','Location_Delhi','Location_Chennai'], axis=1)\n",
    "y_trainf = df['Price']\n",
    "\n",
    "\n",
    "\n",
    "RF = RandomForestRegressor(n_estimators=20,random_state=0,criterion='mae')\n",
    "RF.fit(X_trainf,y_trainf)\n",
    "\n",
    "\n",
    "test_pred=RF.predict(dfte)\n",
    "test_pred=pd.DataFrame(test_pred,columns=['test_data_f_pred'])\n",
    "\n",
    "test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
